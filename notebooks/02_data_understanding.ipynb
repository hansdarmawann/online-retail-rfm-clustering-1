{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TDSP Stage 2 – Data Acquisition & Understanding\n",
        "\n",
        "## Proyek: Segmentasi Pelanggan Online Retail Berbasis RFM (Spark)\n",
        "\n",
        "Stage ini berfokus pada proses **pengambilan data**, **pemahaman struktur data**, serta **eksplorasi awal** menggunakan Apache Spark.\n",
        "Dataset yang digunakan berada dalam format **Parquet**, sehingga efisien untuk diproses secara terdistribusi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Deskripsi Dataset\n",
        "\n",
        "Dataset berisi **data transaksi online retail** dengan karakteristik utama:\n",
        "\n",
        "- Setiap baris merepresentasikan **item produk** dalam suatu transaksi\n",
        "- Satu transaksi dapat terdiri dari banyak item\n",
        "- Pelanggan diidentifikasi menggunakan **PostCode** sebagai *proxy customer ID*\n",
        "\n",
        "Kolom utama yang relevan untuk analisis RFM:\n",
        "- `Invoice`\n",
        "- `StockCode`\n",
        "- `Quantity`\n",
        "- `Price`\n",
        "- `InvoiceDate`\n",
        "- `PostCode`\n",
        "- `Country`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://HD-LP-1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>RFM-Customer-Segmentation</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x272be9d4510>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Inisialisasi Spark Session\n",
        "# ==============================================================\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"RFM-Customer-Segmentation\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Load Data dari Parquet\n",
        "\n",
        "Data transaksi disimpan dalam format **Parquet** pada data lake / storage.\n",
        "Format ini dipilih karena:\n",
        "- Columnar storage (lebih cepat untuk agregasi)\n",
        "- Mendukung schema evolution\n",
        "- Sangat optimal untuk Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: c:\\Users\\U1\\Documents\\online-retail-rfm-clustering-1\\notebooks\n",
            "root\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
            "+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850.0   |United Kingdom|\n",
            "|WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850.0   |United Kingdom|\n",
            "|CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850.0   |United Kingdom|\n",
            "|KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850.0   |United Kingdom|\n",
            "|RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850.0   |United Kingdom|\n",
            "+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# TDSP Stage 2 – Load Data Transaksi dari CSV.GZ (OPSI A)\n",
        "# Notebook location : /notebooks\n",
        "# Dataset location  : /datasets\n",
        "# ==============================================================\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Pastikan SparkSession aktif\n",
        "# --------------------------------------------------------------\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"RFM-Customer-Segmentation\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Debug: cek working directory (penting untuk path relatif)\n",
        "# --------------------------------------------------------------\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Karena notebook berada di folder /notebooks,\n",
        "# maka kita naik 1 level ke project root\n",
        "# --------------------------------------------------------------\n",
        "DATA_PATH = \"../datasets/online_retail_raw.csv.gz\"\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Validasi path sebelum load (anti PATH_NOT_FOUND)\n",
        "# --------------------------------------------------------------\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Path tidak ditemukan: {DATA_PATH}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Load CSV.GZ (Spark auto-handle gzip)\n",
        "# --------------------------------------------------------------\n",
        "df_raw = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .option(\"mode\", \"FAILFAST\")\n",
        "    .csv(DATA_PATH)\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Inspect schema & sample data\n",
        "# --------------------------------------------------------------\n",
        "df_raw.printSchema()\n",
        "df_raw.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Pemahaman Skema & Struktur Data\n",
        "\n",
        "Langkah awal yang penting adalah memahami:\n",
        "- Tipe data setiap kolom\n",
        "- Apakah terdapat kolom yang tidak relevan\n",
        "- Konsistensi format tanggal dan numerik\n",
        "\n",
        "Pemahaman ini akan sangat mempengaruhi proses **feature engineering RFM** pada stage berikutnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+-----------+---------+----------+-------+\n",
            "|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
            "+-----------+--------+-----------+---------+----------+-------+\n",
            "|540455     |541909  |541909     |541909   |406829    |541909 |\n",
            "+-----------+--------+-----------+---------+----------+-------+\n",
            "\n",
            "Jumlah baris data: 541909\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Statistik dasar & jumlah baris\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "df_raw.select([count(c).alias(c) for c in df_raw.columns]).show(truncate=False)\n",
        "\n",
        "print(f\"Jumlah baris data: {df_raw.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Pemeriksaan Kualitas Data (Data Quality Check)\n",
        "\n",
        "Beberapa aspek kualitas data yang perlu diperiksa:\n",
        "- Nilai kosong (null) pada kolom penting\n",
        "- Transaksi tanpa PostCode\n",
        "- Transaksi dari negara di luar scope analisis\n",
        "\n",
        "Tahap ini bersifat **diagnostik**, belum melakukan pembersihan permanen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+-----------+---------+----------+-------+\n",
            "|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
            "+-----------+--------+-----------+---------+----------+-------+\n",
            "|1454       |0       |0          |0        |135080    |0      |\n",
            "+-----------+--------+-----------+---------+----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cek jumlah nilai null per kolom\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "null_summary = df_raw.select([\n",
        "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
        "    for c in df_raw.columns\n",
        "])\n",
        "\n",
        "null_summary.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Filter Awal Berdasarkan Kebutuhan Bisnis\n",
        "\n",
        "Sesuai dengan ruang lingkup proyek:\n",
        "- Hanya transaksi dengan **PostCode valid** yang dipertahankan\n",
        "- Analisis difokuskan pada **pelanggan dari UK**\n",
        "\n",
        "Filter ini bersifat **sementara**, hasil akhirnya akan digunakan pada TDSP Stage 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jumlah baris setelah filter awal: 361878\n",
            "+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
            "+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850.0   |United Kingdom|\n",
            "|WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850.0   |United Kingdom|\n",
            "|CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850.0   |United Kingdom|\n",
            "|KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850.0   |United Kingdom|\n",
            "|RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850.0   |United Kingdom|\n",
            "+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_filtered = (\n",
        "    df_raw\n",
        "    .filter(col(\"CustomerID\").isNotNull())\n",
        "    .filter(col(\"Country\") == \"United Kingdom\")\n",
        ")\n",
        "\n",
        "print(f\"Jumlah baris setelah filter awal: {df_filtered.count()}\")\n",
        "df_filtered.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7027a05a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Stage 2 saved via pandas (csv.gz)\n"
          ]
        }
      ],
      "source": [
        "# Ambil subset yang sudah difilter\n",
        "df_filtered_pd = df_filtered.toPandas()\n",
        "\n",
        "# Simpan pakai pandas (BUKAN spark) → CSV.GZ\n",
        "df_filtered_pd.to_csv(\n",
        "    \"../datasets/online_retail_stage2.csv.gz\",\n",
        "    index=False,\n",
        "    compression=\"gzip\"\n",
        ")\n",
        "\n",
        "print(\"✅ Stage 2 saved via pandas (csv.gz)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Insight Awal (Initial Observations)\n",
        "\n",
        "Beberapa insight awal yang biasanya muncul pada tahap ini:\n",
        "- Distribusi jumlah item per transaksi cenderung tidak merata\n",
        "- Sebagian kecil pelanggan berkontribusi pada volume transaksi yang besar\n",
        "- Terdapat indikasi pelanggan organisasi (B2B) dengan quantity tinggi\n",
        "\n",
        "Insight ini akan diperdalam melalui perhitungan **Recency, Frequency, dan Monetary** pada stage berikutnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ringkasan Stage 2\n",
        "\n",
        "Pada TDSP Stage 2 ini, kita telah:\n",
        "- Memuat data transaksi dari format Parquet menggunakan Spark\n",
        "- Memahami struktur dan skema data\n",
        "- Melakukan pemeriksaan kualitas data awal\n",
        "- Menerapkan filter dasar sesuai kebutuhan bisnis\n",
        "\n",
        "Tahap selanjutnya adalah **TDSP Stage 3 – Modeling**, yang mencakup:\n",
        "- Feature engineering RFM\n",
        "- Normalisasi data\n",
        "- Clustering pelanggan berbasis Spark"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rfmc1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
