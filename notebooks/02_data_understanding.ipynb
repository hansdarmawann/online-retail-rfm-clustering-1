{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TDSP Stage 2 – Data Acquisition & Understanding\n",
        "\n",
        "## Proyek: Segmentasi Pelanggan Online Retail Berbasis RFM (Spark)\n",
        "\n",
        "Stage ini berfokus pada proses **pengambilan data**, **pemahaman struktur data**, serta **eksplorasi awal** menggunakan Apache Spark.\n",
        "Dataset yang digunakan berada dalam format **Parquet**, sehingga efisien untuk diproses secara terdistribusi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Deskripsi Dataset\n",
        "\n",
        "Dataset berisi **data transaksi online retail** dengan karakteristik utama:\n",
        "\n",
        "- Setiap baris merepresentasikan **item produk** dalam suatu transaksi\n",
        "- Satu transaksi dapat terdiri dari banyak item\n",
        "- Pelanggan diidentifikasi menggunakan **PostCode** sebagai *proxy customer ID*\n",
        "\n",
        "Kolom utama yang relevan untuk analisis RFM:\n",
        "- `Invoice`\n",
        "- `StockCode`\n",
        "- `Quantity`\n",
        "- `Price`\n",
        "- `InvoiceDate`\n",
        "- `PostCode`\n",
        "- `Country`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Inisialisasi Spark Session\n",
        "# ==============================================================\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"RFM-Customer-Segmentation\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Load Data dari Parquet\n",
        "\n",
        "Data transaksi disimpan dalam format **Parquet** pada data lake / storage.\n",
        "Format ini dipilih karena:\n",
        "- Columnar storage (lebih cepat untuk agregasi)\n",
        "- Mendukung schema evolution\n",
        "- Sangat optimal untuk Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Load dataset transaksi dari Parquet\n",
        "# ==============================================================\n",
        "# Catatan:\n",
        "# - Sesuaikan path dengan environment (local / Fabric / Databricks)\n",
        "\n",
        "DATA_PATH = \"/data/online_retail/transactions.parquet\"\n",
        "\n",
        "df_raw = spark.read.parquet(DATA_PATH)\n",
        "\n",
        "df_raw.printSchema()\n",
        "df_raw.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Pemahaman Skema & Struktur Data\n",
        "\n",
        "Langkah awal yang penting adalah memahami:\n",
        "- Tipe data setiap kolom\n",
        "- Apakah terdapat kolom yang tidak relevan\n",
        "- Konsistensi format tanggal dan numerik\n",
        "\n",
        "Pemahaman ini akan sangat mempengaruhi proses **feature engineering RFM** pada stage berikutnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Statistik dasar & jumlah baris\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "df_raw.select([count(c).alias(c) for c in df_raw.columns]).show(truncate=False)\n",
        "\n",
        "print(f\"Jumlah baris data: {df_raw.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Pemeriksaan Kualitas Data (Data Quality Check)\n",
        "\n",
        "Beberapa aspek kualitas data yang perlu diperiksa:\n",
        "- Nilai kosong (null) pada kolom penting\n",
        "- Transaksi tanpa PostCode\n",
        "- Transaksi dari negara di luar scope analisis\n",
        "\n",
        "Tahap ini bersifat **diagnostik**, belum melakukan pembersihan permanen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Cek jumlah nilai null per kolom\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "null_summary = df_raw.select([\n",
        "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
        "    for c in df_raw.columns\n",
        "])\n",
        "\n",
        "null_summary.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Filter Awal Berdasarkan Kebutuhan Bisnis\n",
        "\n",
        "Sesuai dengan ruang lingkup proyek:\n",
        "- Hanya transaksi dengan **PostCode valid** yang dipertahankan\n",
        "- Analisis difokuskan pada **pelanggan dari UK**\n",
        "\n",
        "Filter ini bersifat **sementara**, hasil akhirnya akan digunakan pada TDSP Stage 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Filter transaksi valid (PostCode tidak null & UK only)\n",
        "# ==============================================================\n",
        "df_filtered = (\n",
        "    df_raw\n",
        "    .filter(col(\"PostCode\").isNotNull())\n",
        "    .filter(col(\"Country\") == \"United Kingdom\")\n",
        ")\n",
        "\n",
        "print(f\"Jumlah baris setelah filter awal: {df_filtered.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Insight Awal (Initial Observations)\n",
        "\n",
        "Beberapa insight awal yang biasanya muncul pada tahap ini:\n",
        "- Distribusi jumlah item per transaksi cenderung tidak merata\n",
        "- Sebagian kecil pelanggan berkontribusi pada volume transaksi yang besar\n",
        "- Terdapat indikasi pelanggan organisasi (B2B) dengan quantity tinggi\n",
        "\n",
        "Insight ini akan diperdalam melalui perhitungan **Recency, Frequency, dan Monetary** pada stage berikutnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ringkasan Stage 2\n",
        "\n",
        "Pada TDSP Stage 2 ini, kita telah:\n",
        "- Memuat data transaksi dari format Parquet menggunakan Spark\n",
        "- Memahami struktur dan skema data\n",
        "- Melakukan pemeriksaan kualitas data awal\n",
        "- Menerapkan filter dasar sesuai kebutuhan bisnis\n",
        "\n",
        "Tahap selanjutnya adalah **TDSP Stage 3 – Modeling**, yang mencakup:\n",
        "- Feature engineering RFM\n",
        "- Normalisasi data\n",
        "- Clustering pelanggan berbasis Spark"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (PySpark)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
