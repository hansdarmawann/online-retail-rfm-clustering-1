{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a3e3146",
      "metadata": {},
      "source": [
        "# TDSP Stage 3 – Modeling\n",
        "\n",
        "## Proyek: Segmentasi Pelanggan Online Retail Berbasis RFM (Spark)\n",
        "\n",
        "Stage ini berfokus pada **feature engineering RFM**, **transformasi data**, dan **clustering pelanggan** menggunakan Apache Spark.\n",
        "Hasil dari stage ini adalah segmen pelanggan yang bermakna dan dapat diinterpretasikan secara bisnis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d69e2b2",
      "metadata": {},
      "source": [
        "## 3.1 Tujuan Modeling\n",
        "\n",
        "Tujuan utama tahap modeling adalah:\n",
        "\n",
        "1. Mengubah data transaksi menjadi **fitur RFM** pada level pelanggan.\n",
        "2. Menyiapkan dataset yang siap untuk algoritma clustering.\n",
        "3. Mengelompokkan pelanggan berdasarkan kemiripan perilaku transaksi.\n",
        "4. Menghasilkan segmentasi yang **interpretable** dan **actionable** bagi bisnis.\n",
        "\n",
        "Pendekatan yang digunakan bersifat **unsupervised learning**, karena tidak terdapat label target."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afd205d",
      "metadata": {},
      "source": [
        "## 3.2 Persiapan Data Modeling\n",
        "\n",
        "Dataset yang digunakan adalah hasil filter dari TDSP Stage 2.\n",
        "Langkah awal:\n",
        "- Pastikan kolom tanggal dalam format timestamp\n",
        "- Hitung nilai transaksi (Amount)\n",
        "- Tentukan *reference date* untuk perhitungan Recency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "308e2790",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Import fungsi yang dibutuhkan\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import (\n",
        "    col, to_timestamp, max as spark_max,\n",
        "    sum as spark_sum, countDistinct,\n",
        "    datediff\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944e25e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Pastikan InvoiceDate bertipe timestamp\n",
        "# ==============================================================\n",
        "df = df_filtered.withColumn(\n",
        "    \"InvoiceDate\",\n",
        "    to_timestamp(col(\"InvoiceDate\"))\n",
        ")\n",
        "\n",
        "df.select(\"InvoiceDate\").printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e9a76f",
      "metadata": {},
      "source": [
        "## 3.3 Feature Engineering – Monetary\n",
        "\n",
        "**Monetary** merepresentasikan total nilai uang yang dibelanjakan pelanggan.\n",
        "\n",
        "Langkah:\n",
        "- Hitung `Amount = Quantity × Price`\n",
        "- Agregasi total Amount per pelanggan (PostCode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbe7eae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Hitung Amount per baris transaksi\n",
        "# ==============================================================\n",
        "df = df.withColumn(\n",
        "    \"Amount\",\n",
        "    col(\"Quantity\") * col(\"Price\")\n",
        ")\n",
        "\n",
        "df.select(\"Quantity\", \"Price\", \"Amount\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b8a971",
      "metadata": {},
      "source": [
        "## 3.4 Feature Engineering – Recency & Frequency\n",
        "\n",
        "- **Recency**: jarak waktu (hari) antara transaksi terakhir pelanggan dengan tanggal referensi\n",
        "- **Frequency**: jumlah transaksi unik (Invoice) per pelanggan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd109d04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Tentukan reference date (tanggal transaksi terakhir di dataset)\n",
        "# ==============================================================\n",
        "reference_date = df.select(spark_max(\"InvoiceDate\")).collect()[0][0]\n",
        "\n",
        "reference_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ab1065",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Agregasi RFM per pelanggan (PostCode)\n",
        "# ==============================================================\n",
        "rfm_df = (\n",
        "    df.groupBy(\"PostCode\")\n",
        "      .agg(\n",
        "          datediff(reference_date, spark_max(\"InvoiceDate\")).alias(\"Recency\"),\n",
        "          countDistinct(\"Invoice\").alias(\"Frequency\"),\n",
        "          spark_sum(\"Amount\").alias(\"Monetary\")\n",
        "      )\n",
        ")\n",
        "\n",
        "rfm_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2053087a",
      "metadata": {},
      "source": [
        "## 3.5 Pemeriksaan Distribusi RFM\n",
        "\n",
        "Distribusi RFM biasanya **skewed**, khususnya Monetary.\n",
        "Pemeriksaan statistik deskriptif penting sebelum scaling dan clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e88bf73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Statistik deskriptif RFM\n",
        "# ==============================================================\n",
        "rfm_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8afb41",
      "metadata": {},
      "source": [
        "## 3.6 Feature Scaling\n",
        "\n",
        "Karena perbedaan skala antar fitur RFM cukup besar,\n",
        "maka diperlukan **normalisasi / scaling** sebelum clustering.\n",
        "\n",
        "Metode yang digunakan:\n",
        "- `StandardScaler` (mean = 0, std = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d47ae3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Vectorization & Scaling\n",
        "# ==============================================================\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Recency\", \"Frequency\", \"Monetary\"],\n",
        "    outputCol=\"rfm_features\"\n",
        ")\n",
        "\n",
        "rfm_vector = assembler.transform(rfm_df)\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"rfm_features\",\n",
        "    outputCol=\"rfm_scaled\",\n",
        "    withMean=True,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "scaler_model = scaler.fit(rfm_vector)\n",
        "rfm_scaled_df = scaler_model.transform(rfm_vector)\n",
        "\n",
        "rfm_scaled_df.select(\"rfm_scaled\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c5b8e0",
      "metadata": {},
      "source": [
        "## 3.7 Clustering Pelanggan (K-Means)\n",
        "\n",
        "Algoritma **K-Means** digunakan untuk mengelompokkan pelanggan berdasarkan fitur RFM.\n",
        "\n",
        "Jumlah cluster ditentukan berdasarkan:\n",
        "- Interpretabilitas bisnis\n",
        "- Eksperimen awal\n",
        "\n",
        "Pada tahap ini digunakan **k = 5**, sesuai praktik umum segmentasi RFM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efd3c27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# K-Means Clustering\n",
        "# ==============================================================\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "kmeans = KMeans(\n",
        "    featuresCol=\"rfm_scaled\",\n",
        "    predictionCol=\"cluster\",\n",
        "    k=5,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "kmeans_model = kmeans.fit(rfm_scaled_df)\n",
        "rfm_clustered = kmeans_model.transform(rfm_scaled_df)\n",
        "\n",
        "rfm_clustered.select(\"PostCode\", \"Recency\", \"Frequency\", \"Monetary\", \"cluster\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053ad804",
      "metadata": {},
      "source": [
        "## 3.8 Interpretasi Awal Cluster\n",
        "\n",
        "Langkah awal interpretasi:\n",
        "- Hitung statistik RFM per cluster\n",
        "- Bandingkan karakteristik antar cluster\n",
        "\n",
        "Interpretasi ini akan menjadi dasar rekomendasi bisnis pada stage selanjutnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d4a2d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Statistik RFM per cluster\n",
        "# ==============================================================\n",
        "rfm_clustered.groupBy(\"cluster\").agg(\n",
        "    spark_sum(\"Monetary\").alias(\"Total_Monetary\"),\n",
        "    spark_max(\"Monetary\").alias(\"Max_Monetary\"),\n",
        "    spark_max(\"Frequency\").alias(\"Max_Frequency\"),\n",
        "    spark_max(\"Recency\").alias(\"Max_Recency\"),\n",
        "    spark_sum(\"Frequency\").alias(\"Total_Frequency\")\n",
        ").orderBy(\"cluster\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c19c164",
      "metadata": {},
      "source": [
        "## Ringkasan Stage 3\n",
        "\n",
        "Pada TDSP Stage 3 ini, kita telah:\n",
        "- Melakukan feature engineering RFM menggunakan Spark\n",
        "- Melakukan scaling fitur untuk clustering\n",
        "- Mengelompokkan pelanggan menggunakan K-Means\n",
        "- Menghasilkan segmentasi pelanggan awal\n",
        "\n",
        "Tahap berikutnya adalah **TDSP Stage 4 – Deployment**,\n",
        "yang berfokus pada penyajian hasil, insight bisnis, dan output yang siap dikonsumsi stakeholder."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (PySpark)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
