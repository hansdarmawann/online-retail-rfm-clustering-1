{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a3e3146",
      "metadata": {},
      "source": [
        "# TDSP Stage 3 – Modeling\n",
        "\n",
        "## Proyek: Segmentasi Pelanggan Online Retail Berbasis RFM (Spark)\n",
        "\n",
        "Stage ini berfokus pada **feature engineering RFM**, **transformasi data**, dan **clustering pelanggan** menggunakan Apache Spark.\n",
        "Hasil dari stage ini adalah segmen pelanggan yang bermakna dan dapat diinterpretasikan secara bisnis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d69e2b2",
      "metadata": {},
      "source": [
        "## 3.1 Tujuan Modeling\n",
        "\n",
        "Tujuan utama tahap modeling adalah:\n",
        "\n",
        "1. Mengubah data transaksi menjadi **fitur RFM** pada level pelanggan.\n",
        "2. Menyiapkan dataset yang siap untuk algoritma clustering.\n",
        "3. Mengelompokkan pelanggan berdasarkan kemiripan perilaku transaksi.\n",
        "4. Menghasilkan segmentasi yang **interpretable** dan **actionable** bagi bisnis.\n",
        "\n",
        "Pendekatan yang digunakan bersifat **unsupervised learning**, karena tidak terdapat label target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "525c84c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 3.1 Inisialisasi Spark Session (Wajib di awal)\n",
        "# ==============================================================\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Inisialisasi Spark\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"RFM-Customer-Segmentation-Stage3\")\n",
        "    .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afd205d",
      "metadata": {},
      "source": [
        "## 3.2 Persiapan Data Modeling\n",
        "\n",
        "Dataset yang digunakan adalah hasil filter dari TDSP Stage 2.\n",
        "Langkah awal:\n",
        "- Pastikan kolom tanggal dalam format timestamp\n",
        "- Hitung nilai transaksi (Amount)\n",
        "- Tentukan *reference date* untuk perhitungan Recency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d3361a69",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Loading Parquet via Pandas...\n",
            "2. Saving to Temp CSV...\n",
            "3. Loading CSV into Spark...\n",
            "4. Caching data to memory...\n",
            "Data loaded successfully: 361878 rows\n",
            "   (Temp CSV deleted)\n",
            "root\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n",
            "| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n",
            "|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n",
            "|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n",
            "|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n",
            "+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.2 Load Data & Preprocessing (Metode CSV Bridge - Fixed Lazy Eval)\n",
        "# ==============================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Path Data\n",
        "PARQUET_PATH = \"../datasets/online_retail_stage2.parquet\"\n",
        "TEMP_CSV_PATH = \"../datasets/temp_online_retail.csv\"\n",
        "\n",
        "print(\"1. Loading Parquet via Pandas...\")\n",
        "pdf = pd.read_parquet(PARQUET_PATH)\n",
        "pdf[\"InvoiceDate\"] = pdf[\"InvoiceDate\"].astype(\"datetime64[us]\")\n",
        "\n",
        "print(\"2. Saving to Temp CSV...\")\n",
        "pdf.to_csv(TEMP_CSV_PATH, index=False)\n",
        "\n",
        "print(\"3. Loading CSV into Spark...\")\n",
        "df_temp = spark.read.csv(TEMP_CSV_PATH, header=True, inferSchema=True)\n",
        "\n",
        "# Transformasi Tipe Data\n",
        "df_stage3 = df_temp.withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"))) \\\n",
        "                   .withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"integer\")) \\\n",
        "                   .withColumn(\"Quantity\", col(\"Quantity\").cast(\"integer\")) \\\n",
        "                   .withColumn(\"UnitPrice\", col(\"UnitPrice\").cast(\"double\"))\n",
        "\n",
        "# --- BAGIAN PENTING ---\n",
        "# Kita Cache ke memory & trigger Action (count) DULU sebelum hapus file\n",
        "print(\"4. Caching data to memory...\")\n",
        "df_stage3.cache() \n",
        "row_count = df_stage3.count() # Ini memaksa Spark membaca CSV sekarang juga\n",
        "print(f\"Data loaded successfully: {row_count} rows\")\n",
        "\n",
        "# 5. Sekarang aman untuk hapus file temp\n",
        "if os.path.exists(TEMP_CSV_PATH):\n",
        "    os.remove(TEMP_CSV_PATH)\n",
        "    print(\"   (Temp CSV deleted)\")\n",
        "\n",
        "df_stage3.printSchema()\n",
        "df_stage3.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e9a76f",
      "metadata": {},
      "source": [
        "## 3.3 Feature Engineering – Monetary\n",
        "\n",
        "**Monetary** merepresentasikan total nilai uang yang dibelanjakan pelanggan.\n",
        "\n",
        "Langkah:\n",
        "- Hitung `Amount = Quantity × Price`\n",
        "- Agregasi total Amount per pelanggan (PostCode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "91957111",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------+\n",
            "|CustomerID|Monetary|\n",
            "+----------+--------+\n",
            "|     17420|  598.83|\n",
            "|     16861|  151.65|\n",
            "|     16503| 1421.43|\n",
            "|     15727| 5178.96|\n",
            "|     17389|31300.08|\n",
            "+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.3 Hitung Monetary\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import sum as spark_sum, round as spark_round\n",
        "\n",
        "# 1. Hitung total per baris transaksi\n",
        "df_stage3 = df_stage3.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
        "\n",
        "# 2. Group by CustomerID untuk mendapatkan Monetary Value\n",
        "monetary_df = df_stage3.groupBy(\"CustomerID\") \\\n",
        "    .agg(spark_round(spark_sum(\"TotalPrice\"), 2).alias(\"Monetary\"))\n",
        "\n",
        "monetary_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b8a971",
      "metadata": {},
      "source": [
        "## 3.4 Feature Engineering – Recency & Frequency\n",
        "\n",
        "- **Recency**: jarak waktu (hari) antara transaksi terakhir pelanggan dengan tanggal referensi\n",
        "- **Frequency**: jumlah transaksi unik (Invoice) per pelanggan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8336ca8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last Transaction Date in Data: 2011-12-09 12:49:00\n",
            "Sample Data RFM:\n",
            "+----------+---------+-------+-----------------+\n",
            "|CustomerID|Frequency|Recency|         Monetary|\n",
            "+----------+---------+-------+-----------------+\n",
            "|     14450|        3|    180|           483.25|\n",
            "|     16861|        3|     59|           151.65|\n",
            "|     17389|       43|      0|         31300.08|\n",
            "|     17420|        3|     50|598.8299999999999|\n",
            "|     16503|        5|    106|          1421.43|\n",
            "+----------+---------+-------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Total Unique Customers: 3950\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.4 Hitung Recency & Frequency (Fix Missing InvoiceNo)\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import max as spark_max, countDistinct, datediff, lit, col\n",
        "\n",
        "# 1. Tentukan Reference Date (Max Date di dataset + 1 hari)\n",
        "max_date_row = df_stage3.agg(spark_max(\"InvoiceDate\")).collect()[0][0]\n",
        "print(f\"Last Transaction Date in Data: {max_date_row}\")\n",
        "\n",
        "# 2. Hitung Recency & Frequency\n",
        "#    CATATAN: Kita gunakan countDistinct(\"InvoiceDate\") sebagai pengganti \"InvoiceNo\"\n",
        "rf_df = df_stage3.groupBy(\"CustomerID\") \\\n",
        "    .agg(\n",
        "        spark_max(\"InvoiceDate\").alias(\"LastPurchaseDate\"),\n",
        "        countDistinct(\"InvoiceDate\").alias(\"Frequency\") \n",
        "    ) \\\n",
        "    .withColumn(\"Recency\", datediff(lit(max_date_row), col(\"LastPurchaseDate\"))) \\\n",
        "    .drop(\"LastPurchaseDate\")\n",
        "\n",
        "# 3. Pastikan monetary_df ada (Hitung ulang cepat jika variable hilang)\n",
        "#    Ini defensive coding agar cell ini mandiri\n",
        "df_temp_m = df_stage3.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
        "monetary_df = df_temp_m.groupBy(\"CustomerID\").agg(spark_max(\"TotalPrice\").alias(\"Monetary\")) \n",
        "# Note: Idealnya sum(), tapi di snippet ini saya pakai logic sederhana dulu agar jalan. \n",
        "# Koreksi: Mari pakai SUM agar benar secara bisnis.\n",
        "from pyspark.sql.functions import sum as spark_sum\n",
        "monetary_df = df_temp_m.groupBy(\"CustomerID\").agg(spark_sum(\"TotalPrice\").alias(\"Monetary\"))\n",
        "\n",
        "\n",
        "# 4. Gabungkan (Join) RFM menjadi satu DataFrame utama\n",
        "rfm_df = rf_df.join(monetary_df, on=\"CustomerID\", how=\"inner\")\n",
        "\n",
        "# Validasi hasil\n",
        "print(\"Sample Data RFM:\")\n",
        "rfm_df.show(5)\n",
        "print(f\"Total Unique Customers: {rfm_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2053087a",
      "metadata": {},
      "source": [
        "## 3.5 Pemeriksaan Distribusi RFM\n",
        "\n",
        "Distribusi RFM biasanya **skewed**, khususnya Monetary.\n",
        "Pemeriksaan statistik deskriptif penting sebelum scaling dan clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4308e2a4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+-----------------+------------------+\n",
            "|summary|           Recency|        Frequency|          Monetary|\n",
            "+-------+------------------+-----------------+------------------+\n",
            "|  count|              3950|             3950|              3950|\n",
            "|   mean| 91.32303797468354|4.989367088607595|1713.3856693670866|\n",
            "| stddev|100.23684765413593|8.614673469817246| 6548.608224207452|\n",
            "|    min|                 0|                1|          -4287.63|\n",
            "|    25%|                16|                1|282.18999999999994|\n",
            "|    50%|                50|                3|            626.99|\n",
            "|    75%|               143|                5|           1521.79|\n",
            "|    max|               373|              225|256438.48999999996|\n",
            "+-------+------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.5 Statistik Deskriptif RFM\n",
        "# ==============================================================\n",
        "# Cek statistik dasar untuk melihat range dan skewness\n",
        "rfm_df.select(\"Recency\", \"Frequency\", \"Monetary\").summary().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8afb41",
      "metadata": {},
      "source": [
        "## 3.6 Feature Scaling\n",
        "\n",
        "Karena perbedaan skala antar fitur RFM cukup besar,\n",
        "maka diperlukan **normalisasi / scaling** sebelum clustering.\n",
        "\n",
        "Metode yang digunakan:\n",
        "- `StandardScaler` (mean = 0, std = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b5b73f53",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------------------------------------------------------+\n",
            "|CustomerID|scaled_features                                                 |\n",
            "+----------+----------------------------------------------------------------+\n",
            "|14450     |[0.8846742899506721,-0.23092774155371382,-0.18784688704078964]  |\n",
            "|16861     |[-0.32246662510989116,-0.23092774155371382,-0.2384836007739786] |\n",
            "|17389     |[-0.9110725258419013,4.412312671463191,4.518012578804657]       |\n",
            "|17420     |[-0.41225396589951985,-0.23092774155371382,-0.17019733525164013]|\n",
            "|16503     |[0.1464228212359475,0.0012342790971314125,-0.04458285781822258] |\n",
            "+----------+----------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.6 Vector Assembly & Standard Scaling\n",
        "# ==============================================================\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# 1. Assemble fitur menjadi satu kolom vektor\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Recency\", \"Frequency\", \"Monetary\"],\n",
        "    outputCol=\"features_vec\"\n",
        ")\n",
        "\n",
        "# 2. Standard Scaler\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_vec\",\n",
        "    outputCol=\"scaled_features\",\n",
        "    withStd=True,\n",
        "    withMean=True\n",
        ")\n",
        "\n",
        "# 3. Buat Pipeline Preprocessing\n",
        "pipeline_prep = Pipeline(stages=[assembler, scaler])\n",
        "model_prep = pipeline_prep.fit(rfm_df)\n",
        "rfm_scaled = model_prep.transform(rfm_df)\n",
        "\n",
        "rfm_scaled.select(\"CustomerID\", \"scaled_features\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c5b8e0",
      "metadata": {},
      "source": [
        "## 3.7 Clustering Pelanggan (K-Means)\n",
        "\n",
        "Algoritma **K-Means** digunakan untuk mengelompokkan pelanggan berdasarkan fitur RFM.\n",
        "\n",
        "Jumlah cluster ditentukan berdasarkan:\n",
        "- Interpretabilitas bisnis\n",
        "- Eksperimen awal\n",
        "\n",
        "Pada tahap ini digunakan **k = 5**, sesuai praktik umum segmentasi RFM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ddf0927c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+---------+------------------+----------+\n",
            "|CustomerID|Recency|Frequency|          Monetary|prediction|\n",
            "+----------+-------+---------+------------------+----------+\n",
            "|     14450|    180|        3|            483.25|         2|\n",
            "|     16861|     59|        3|            151.65|         3|\n",
            "|     17389|      0|       43|          31300.08|         0|\n",
            "|     17420|     50|        3| 598.8299999999999|         3|\n",
            "|     16503|    106|        5|           1421.43|         3|\n",
            "|     15447|    330|        1|            155.17|         2|\n",
            "|     15727|     16|        7| 5178.960000000001|         1|\n",
            "|     13623|     30|        7|            672.44|         1|\n",
            "|     13285|     23|        4|           2709.12|         1|\n",
            "|     14570|    280|        2|218.05999999999992|         2|\n",
            "+----------+-------+---------+------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.7 Training K-Means\n",
        "# ==============================================================\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Inisialisasi K-Means\n",
        "kmeans = KMeans(featuresCol=\"scaled_features\", k=5, seed=42)\n",
        "\n",
        "# Training Model\n",
        "model_kmeans = kmeans.fit(rfm_scaled)\n",
        "\n",
        "# Prediksi Cluster\n",
        "predictions = model_kmeans.transform(rfm_scaled)\n",
        "\n",
        "# Menampilkan hasil prediksi (Cluster 0 s/d 4)\n",
        "predictions.select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"prediction\").show(10)\n",
        "\n",
        "# Simpan hasil prediksi untuk Stage 4 (Deployment/Analisis Lanjutan)\n",
        "# predictions.write.parquet(\"../datasets/rfm_clustered.parquet\", mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053ad804",
      "metadata": {},
      "source": [
        "## 3.8 Interpretasi Awal Cluster\n",
        "\n",
        "Langkah awal interpretasi:\n",
        "- Hitung statistik RFM per cluster\n",
        "- Bandingkan karakteristik antar cluster\n",
        "\n",
        "Interpretasi ini akan menjadi dasar rekomendasi bisnis pada stage selanjutnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1a0f6be0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------+-----------+-------------+------------+\n",
            "|prediction|Num_Customers|Avg_Recency|Avg_Frequency|Avg_Monetary|\n",
            "+----------+-------------+-----------+-------------+------------+\n",
            "|         0|           50|        5.0|         53.8|     29624.7|\n",
            "|         1|         1219|       18.0|          9.2|      2724.9|\n",
            "|         2|          841|      262.0|          1.8|      418.35|\n",
            "|         3|         1838|       65.0|          2.3|      636.15|\n",
            "|         4|            2|        4.0|         56.0|   221960.33|\n",
            "+----------+-------------+-----------+-------------+------------+\n",
            "\n",
            "Interpretasi (Contoh Generic):\n",
            "- Cluster dengan Recency Rendah, Freq & Monetary Tinggi = 'Champions'\n",
            "- Cluster dengan Recency Tinggi, Freq & Monetary Rendah = 'Lost/Churned'\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.8 Cluster Profiling (Analisis Rata-rata)\n",
        "# ==============================================================\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "# Hitung rata-rata R, F, M dan jumlah member per cluster\n",
        "cluster_analysis = predictions.groupBy(\"prediction\") \\\n",
        "    .agg(\n",
        "        count(\"CustomerID\").alias(\"Num_Customers\"),\n",
        "        spark_round(avg(\"Recency\"), 0).alias(\"Avg_Recency\"),\n",
        "        spark_round(avg(\"Frequency\"), 1).alias(\"Avg_Frequency\"),\n",
        "        spark_round(avg(\"Monetary\"), 2).alias(\"Avg_Monetary\")\n",
        "    ) \\\n",
        "    .orderBy(\"prediction\")\n",
        "\n",
        "cluster_analysis.show()\n",
        "\n",
        "# Penjelasan singkat (Manual Inspection based on output):\n",
        "print(\"Interpretasi (Contoh Generic):\")\n",
        "print(\"- Cluster dengan Recency Rendah, Freq & Monetary Tinggi = 'Champions'\")\n",
        "print(\"- Cluster dengan Recency Tinggi, Freq & Monetary Rendah = 'Lost/Churned'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c19c164",
      "metadata": {},
      "source": [
        "## Ringkasan Stage 3\n",
        "\n",
        "Pada TDSP Stage 3 ini, kita telah:\n",
        "- Melakukan feature engineering RFM menggunakan Spark\n",
        "- Melakukan scaling fitur untuk clustering\n",
        "- Mengelompokkan pelanggan menggunakan K-Means\n",
        "- Menghasilkan segmentasi pelanggan awal\n",
        "\n",
        "Tahap berikutnya adalah **TDSP Stage 4 – Deployment**,\n",
        "yang berfokus pada penyajian hasil, insight bisnis, dan output yang siap dikonsumsi stakeholder."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rfmc1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
