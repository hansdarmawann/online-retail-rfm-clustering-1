{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a3e3146",
      "metadata": {},
      "source": [
        "# TDSP Stage 3 – Modeling\n",
        "\n",
        "## Proyek: Segmentasi Pelanggan Online Retail Berbasis RFM (Spark)\n",
        "\n",
        "Stage ini berfokus pada **feature engineering RFM**, **transformasi data**, dan **clustering pelanggan** menggunakan Apache Spark.\n",
        "Hasil dari stage ini adalah segmen pelanggan yang bermakna dan dapat diinterpretasikan secara bisnis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d69e2b2",
      "metadata": {},
      "source": [
        "## 3.1 Tujuan Modeling\n",
        "\n",
        "Tujuan utama tahap modeling adalah:\n",
        "\n",
        "1. Mengubah data transaksi menjadi **fitur RFM** pada level pelanggan.\n",
        "2. Menyiapkan dataset yang siap untuk algoritma clustering.\n",
        "3. Mengelompokkan pelanggan berdasarkan kemiripan perilaku transaksi.\n",
        "4. Menghasilkan segmentasi yang **interpretable** dan **actionable** bagi bisnis.\n",
        "\n",
        "Pendekatan yang digunakan bersifat **unsupervised learning**, karena tidak terdapat label target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "83f59c08",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Windows Hadoop Fix (WAJIB untuk write Parquet)\n",
        "# ==============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
        "os.environ[\"hadoop.home.dir\"] = r\"C:\\hadoop\"\n",
        "os.environ[\"PATH\"] += r\";C:\\hadoop\\bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "36233382",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 3.1 Spark Session Initialization\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"RFM-Customer-Segmentation\")\n",
        "    .getOrCreate()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afd205d",
      "metadata": {},
      "source": [
        "## 3.2 Persiapan Data Modeling\n",
        "\n",
        "Dataset yang digunakan adalah hasil filter dari TDSP Stage 2.\n",
        "Langkah awal:\n",
        "- Pastikan kolom tanggal dalam format timestamp\n",
        "- Hitung nilai transaksi (Amount)\n",
        "- Tentukan *reference date* untuk perhitungan Recency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "77666612",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid transactions: 354321\n",
            "+-------------------+\n",
            "|        InvoiceDate|\n",
            "+-------------------+\n",
            "|2010-12-01 08:26:00|\n",
            "|2010-12-01 08:26:00|\n",
            "|2010-12-01 08:26:00|\n",
            "|2010-12-01 08:26:00|\n",
            "|2010-12-01 08:26:00|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.2 Load Data & Basic Preparation (FINAL FIX)\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "DATA_PATH = \"../datasets/online_retail_stage2.csv.gz\"\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Load data\n",
        "# --------------------------------------------------------------\n",
        "df = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(DATA_PATH)\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# FIX: Parse InvoiceDate dengan format eksplisit\n",
        "# Dataset Online Retail umumnya: \"12/1/2010 8:26\"\n",
        "# --------------------------------------------------------------\n",
        "df = (\n",
        "    df\n",
        "    .withColumn(\n",
        "        \"InvoiceDate\",\n",
        "        to_timestamp(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")\n",
        "    )\n",
        "    .withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"string\"))\n",
        "    .withColumn(\"Quantity\", col(\"Quantity\").cast(\"integer\"))\n",
        "    .withColumn(\"UnitPrice\", col(\"UnitPrice\").cast(\"double\"))\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Ambil transaksi VALID untuk RFM\n",
        "# (business rule: pembelian aktual)\n",
        "# --------------------------------------------------------------\n",
        "df_valid = df.filter(\n",
        "    (col(\"Quantity\") > 0) &\n",
        "    (col(\"UnitPrice\") > 0) &\n",
        "    col(\"CustomerID\").isNotNull() &\n",
        "    col(\"InvoiceDate\").isNotNull()   # ⬅️ KRUSIAL\n",
        ")\n",
        "\n",
        "df_valid.cache()\n",
        "\n",
        "print(\"Valid transactions:\", df_valid.count())\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Sanity check (WAJIB sekali)\n",
        "# --------------------------------------------------------------\n",
        "df_valid.select(\"InvoiceDate\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e9a76f",
      "metadata": {},
      "source": [
        "## 3.3 Feature Engineering – Monetary\n",
        "\n",
        "**Monetary** merepresentasikan total nilai uang yang dibelanjakan pelanggan.\n",
        "\n",
        "Langkah:\n",
        "- Hitung `Amount = Quantity × Price`\n",
        "- Agregasi total Amount per pelanggan (PostCode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "508d8a44",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|CustomerID|          Monetary|\n",
            "+----------+------------------+\n",
            "|   15039.0|19914.439999999966|\n",
            "|   13178.0| 5725.470000000003|\n",
            "|   16553.0| 5719.819999999999|\n",
            "|   17786.0|            278.74|\n",
            "|   12891.0|             331.0|\n",
            "+----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.3 Feature Engineering – Monetary\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.sql.functions import sum as spark_sum\n",
        "\n",
        "df_valid = df_valid.withColumn(\n",
        "    \"Amount\", col(\"Quantity\") * col(\"UnitPrice\")\n",
        ")\n",
        "\n",
        "monetary_df = (\n",
        "    df_valid\n",
        "    .groupBy(\"CustomerID\")\n",
        "    .agg(spark_sum(\"Amount\").alias(\"Monetary\"))\n",
        ")\n",
        "\n",
        "monetary_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b8a971",
      "metadata": {},
      "source": [
        "## 3.4 Feature Engineering – Recency & Frequency\n",
        "\n",
        "- **Recency**: jarak waktu (hari) antara transaksi terakhir pelanggan dengan tanggal referensi\n",
        "- **Frequency**: jumlah transaksi unik (Invoice) per pelanggan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "8336ca8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total customers with RFM: 3920\n",
            "+----------+---------+-------+------------------+\n",
            "|CustomerID|Frequency|Recency|          Monetary|\n",
            "+----------+---------+-------+------------------+\n",
            "|   15039.0|       42|      9|19914.439999999966|\n",
            "|   17966.0|        4|     37|           1098.43|\n",
            "|   16553.0|        6|    163| 5719.819999999999|\n",
            "|   17955.0|        3|    198|             557.3|\n",
            "|   13178.0|       11|     26| 5725.470000000003|\n",
            "+----------+---------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.4 Feature Engineering – Recency & Frequency\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.sql.functions import (\n",
        "    max as spark_max,\n",
        "    countDistinct,\n",
        "    datediff,\n",
        "    lit,\n",
        "    to_date,\n",
        "    col\n",
        ")\n",
        "\n",
        "# Reference date = transaksi terakhir di dataset valid\n",
        "ref_date = df_valid.agg(spark_max(\"InvoiceDate\")).collect()[0][0]\n",
        "\n",
        "rf_df = (\n",
        "    df_valid\n",
        "    # turunkan ke level tanggal agar tidak over-count\n",
        "    .withColumn(\"InvoiceDay\", to_date(col(\"InvoiceDate\")))\n",
        "    .groupBy(\"CustomerID\")\n",
        "    .agg(\n",
        "        spark_max(\"InvoiceDate\").alias(\"LastPurchaseDate\"),\n",
        "        countDistinct(\"InvoiceDay\").alias(\"Frequency\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"Recency\",\n",
        "        datediff(lit(ref_date), col(\"LastPurchaseDate\"))\n",
        "    )\n",
        "    .drop(\"LastPurchaseDate\")\n",
        ")\n",
        "\n",
        "# Gabungkan RFM\n",
        "from pyspark.sql.functions import coalesce, lit\n",
        "\n",
        "rfm_df = (\n",
        "    rf_df\n",
        "    .join(monetary_df, on=\"CustomerID\", how=\"left\")\n",
        "    .withColumn(\"Monetary\", coalesce(col(\"Monetary\"), lit(0.0)))\n",
        ")\n",
        "\n",
        "print(\"Total customers with RFM:\", rfm_df.count())\n",
        "rfm_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2053087a",
      "metadata": {},
      "source": [
        "## 3.5 Pemeriksaan Distribusi RFM\n",
        "\n",
        "Distribusi RFM biasanya **skewed**, khususnya Monetary.\n",
        "Pemeriksaan statistik deskriptif penting sebelum scaling dan clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "be490924",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------+-----------------+------------------+\n",
            "|summary|          Recency|        Frequency|          Monetary|\n",
            "+-------+-----------------+-----------------+------------------+\n",
            "|  count|             3920|             3920|              3920|\n",
            "|   mean|91.74209183673469|             3.85|1864.3856005102004|\n",
            "| stddev|99.53348473774295|5.713358015316058| 7482.817476870354|\n",
            "|    min|                0|                1|              3.75|\n",
            "|    25%|               17|                1|300.03999999999996|\n",
            "|    50%|               50|                2| 651.8200000000002|\n",
            "|    75%|              142|                4|1575.8900000000003|\n",
            "|    max|              373|              113|259657.30000000005|\n",
            "+-------+-----------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.5 Pemeriksaan Distribusi RFM\n",
        "# ==============================================================\n",
        "\n",
        "rfm_df.select(\n",
        "    \"Recency\",\n",
        "    \"Frequency\",\n",
        "    \"Monetary\"\n",
        ").summary().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8afb41",
      "metadata": {},
      "source": [
        "## 3.6 Feature Scaling\n",
        "\n",
        "Karena perbedaan skala antar fitur RFM cukup besar,\n",
        "maka diperlukan **normalisasi / scaling** sebelum clustering.\n",
        "\n",
        "Metode yang digunakan:\n",
        "- `StandardScaler` (mean = 0, std = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "e109f36e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RFM rows after cleaning: 3920\n",
            "+----------+---------+-------+------------------+\n",
            "|CustomerID|Frequency|Recency|          Monetary|\n",
            "+----------+---------+-------+------------------+\n",
            "|   15039.0|       42|      9|19914.439999999966|\n",
            "|   17966.0|        4|     37|           1098.43|\n",
            "|   16553.0|        6|    163| 5719.819999999999|\n",
            "|   17955.0|        3|    198|             557.3|\n",
            "|   13178.0|       11|     26| 5725.470000000003|\n",
            "+----------+---------+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+-------+---------+------------------+--------------------------------------------------------------+\n",
            "|CustomerID|Recency|Frequency|Monetary          |scaled_features                                               |\n",
            "+----------+-------+---------+------------------+--------------------------------------------------------------+\n",
            "|15039.0   |9      |42       |19914.439999999966|[-0.8312990553354861,6.677334047285251,2.412200278208456]     |\n",
            "|17966.0   |37     |4        |1098.43           |[-0.5499866902175947,0.026254262309116847,-0.1023619248869559]|\n",
            "|16553.0   |163    |6        |5719.819999999999 |[0.7159189528129164,0.37631109309733446,0.5152383325407943]   |\n",
            "|17955.0   |198    |3        |557.3             |[1.0675594092102805,-0.14877415308499195,-0.17467826851990587]|\n",
            "|13178.0   |26     |11       |5725.470000000003 |[-0.6605022622281949,1.2514531700678784,0.5159933957262146]   |\n",
            "+----------+-------+---------+------------------+--------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.6 Feature Scaling (Option 1: Proper Cleaning)\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "rfm_clean = (\n",
        "    rfm_df\n",
        "    .filter(col(\"Recency\").isNotNull())\n",
        "    .filter(col(\"Frequency\") >= 1)\n",
        "    .filter(col(\"Monetary\") > 0)\n",
        ")\n",
        "\n",
        "print(\"RFM rows after cleaning:\", rfm_clean.count())\n",
        "rfm_clean.show(5)\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Recency\", \"Frequency\", \"Monetary\"],\n",
        "    outputCol=\"features_vec\"\n",
        ")\n",
        "\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_vec\",\n",
        "    outputCol=\"scaled_features\",\n",
        "    withMean=True,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "pipeline_prep = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "prep_model = pipeline_prep.fit(rfm_clean)\n",
        "rfm_scaled = prep_model.transform(rfm_clean)\n",
        "\n",
        "rfm_scaled.select(\n",
        "    \"CustomerID\",\n",
        "    \"Recency\",\n",
        "    \"Frequency\",\n",
        "    \"Monetary\",\n",
        "    \"scaled_features\"\n",
        ").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c5b8e0",
      "metadata": {},
      "source": [
        "## 3.7 Clustering Pelanggan (K-Means)\n",
        "\n",
        "Algoritma **K-Means** digunakan untuk mengelompokkan pelanggan berdasarkan fitur RFM.\n",
        "\n",
        "Jumlah cluster ditentukan berdasarkan:\n",
        "- Interpretabilitas bisnis\n",
        "- Eksperimen awal\n",
        "\n",
        "Pada tahap ini digunakan **k = 5**, sesuai praktik umum segmentasi RFM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "42f91dfc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+---------+------------------+-------+\n",
            "|CustomerID|Recency|Frequency|          Monetary|Cluster|\n",
            "+----------+-------+---------+------------------+-------+\n",
            "|   15039.0|      9|       42|19914.439999999966|      3|\n",
            "|   17966.0|     37|        4|           1098.43|      0|\n",
            "|   16553.0|    163|        6| 5719.819999999999|      2|\n",
            "|   17955.0|    198|        3|             557.3|      2|\n",
            "|   13178.0|     26|       11| 5725.470000000003|      4|\n",
            "|   12985.0|      0|        2|           1239.38|      0|\n",
            "|   16557.0|     24|        2|281.84999999999997|      0|\n",
            "|   17786.0|     85|        2|            278.74|      0|\n",
            "|   13259.0|     61|        1| 292.3199999999999|      0|\n",
            "|   14349.0|     10|        1|133.50000000000006|      0|\n",
            "+----------+-------+---------+------------------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.7 Clustering Pelanggan (K-Means)\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Jumlah cluster (praktik umum RFM)\n",
        "K = 5\n",
        "\n",
        "kmeans = KMeans(\n",
        "    featuresCol=\"scaled_features\",\n",
        "    predictionCol=\"Cluster\",\n",
        "    k=K,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "kmeans_model = kmeans.fit(rfm_scaled)\n",
        "\n",
        "# Assign cluster ke setiap customer\n",
        "rfm_clustered = kmeans_model.transform(rfm_scaled)\n",
        "\n",
        "rfm_clustered.select(\n",
        "    \"CustomerID\",\n",
        "    \"Recency\",\n",
        "    \"Frequency\",\n",
        "    \"Monetary\",\n",
        "    \"Cluster\"\n",
        ").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "9439706d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------+\n",
            "|Cluster|Num_Customers|\n",
            "+-------+-------------+\n",
            "|      0|         2603|\n",
            "|      1|            3|\n",
            "|      2|          962|\n",
            "|      3|           25|\n",
            "|      4|          327|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "rfm_clustered.groupBy(\"Cluster\") \\\n",
        "    .agg(count(\"CustomerID\").alias(\"Num_Customers\")) \\\n",
        "    .orderBy(\"Cluster\") \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "7eb46569",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------------+------------+-------------+\n",
            "|Cluster|Avg_Recency|Avg_Frequency|Avg_Monetary|Num_Customers|\n",
            "+-------+-----------+-------------+------------+-------------+\n",
            "|      0|       44.9|          3.0|     1114.35|         2603|\n",
            "|      1|        2.7|         18.3|    207560.2|            3|\n",
            "|      2|      247.0|          1.5|       493.0|          962|\n",
            "|      3|       17.8|         45.2|     45946.6|           25|\n",
            "|      4|       14.3|         14.1|     6611.98|          327|\n",
            "+-------+-----------+-------------+------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import avg, round as spark_round\n",
        "\n",
        "cluster_profile = (\n",
        "    rfm_clustered\n",
        "    .groupBy(\"Cluster\")\n",
        "    .agg(\n",
        "        spark_round(avg(\"Recency\"), 1).alias(\"Avg_Recency\"),\n",
        "        spark_round(avg(\"Frequency\"), 1).alias(\"Avg_Frequency\"),\n",
        "        spark_round(avg(\"Monetary\"), 2).alias(\"Avg_Monetary\"),\n",
        "        count(\"CustomerID\").alias(\"Num_Customers\")\n",
        "    )\n",
        "    .orderBy(\"Cluster\")\n",
        ")\n",
        "\n",
        "cluster_profile.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053ad804",
      "metadata": {},
      "source": [
        "## 3.8 Interpretasi Awal Cluster\n",
        "\n",
        "Langkah awal interpretasi:\n",
        "- Hitung statistik RFM per cluster\n",
        "- Bandingkan karakteristik antar cluster\n",
        "\n",
        "Interpretasi ini akan menjadi dasar rekomendasi bisnis pada stage selanjutnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "fcae2f0f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------+-----------+-------------+------------+\n",
            "|Cluster|Num_Customers|Avg_Recency|Avg_Frequency|Avg_Monetary|\n",
            "+-------+-------------+-----------+-------------+------------+\n",
            "|      0|         2603|       44.9|          3.0|     1114.35|\n",
            "|      1|            3|        2.7|         18.3|    207560.2|\n",
            "|      2|          962|      247.0|          1.5|       493.0|\n",
            "|      3|           25|       17.8|         45.2|     45946.6|\n",
            "|      4|          327|       14.3|         14.1|     6611.98|\n",
            "+-------+-------------+-----------+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3.8 Cluster Profiling (RFM Statistics)\n",
        "# ==============================================================\n",
        "\n",
        "from pyspark.sql.functions import avg, count, round as spark_round\n",
        "\n",
        "cluster_profile = (\n",
        "    rfm_clustered\n",
        "    .groupBy(\"Cluster\")\n",
        "    .agg(\n",
        "        count(\"CustomerID\").alias(\"Num_Customers\"),\n",
        "        spark_round(avg(\"Recency\"), 1).alias(\"Avg_Recency\"),\n",
        "        spark_round(avg(\"Frequency\"), 1).alias(\"Avg_Frequency\"),\n",
        "        spark_round(avg(\"Monetary\"), 2).alias(\"Avg_Monetary\")\n",
        "    )\n",
        "    .orderBy(\"Cluster\")\n",
        ")\n",
        "\n",
        "cluster_profile.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc79eca2",
      "metadata": {},
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c19c164",
      "metadata": {},
      "source": [
        "## Ringkasan Stage 3\n",
        "\n",
        "Pada TDSP Stage 3 ini, kita telah:\n",
        "- Melakukan feature engineering RFM menggunakan Spark\n",
        "- Melakukan scaling fitur untuk clustering\n",
        "- Mengelompokkan pelanggan menggunakan K-Means\n",
        "- Menghasilkan segmentasi pelanggan awal\n",
        "\n",
        "Tahap berikutnya adalah **TDSP Stage 4 – Deployment**,\n",
        "yang berfokus pada penyajian hasil, insight bisnis, dan output yang siap dikonsumsi stakeholder."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rfmc1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
